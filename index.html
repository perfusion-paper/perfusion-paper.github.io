<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Key-Locked Rank One Editing for Text-to-Image Personalization">
  <meta name="keywords" content="Perfusion, Text-to-Image, Personalized Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Key-Locked Rank One Editing for Text-to-Image Personalization</title>

<!--   Global site tag (gtag.js) - Google Analytics-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5ZVQZ7NHC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-Y5ZVQZ7NHC');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Key-Locked Rank One Editing for <br/> Text-to-Image Personalization</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">
              Anonymous Authors
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2108.00946"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->

              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Colab Demo Link. -->
<!--              <span class="link-block">-->
<!--                <a href="http://colab.research.google.com/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-infinity"></i>-->
<!--                  </span>-->
<!--                  <span>Colab</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Replicate Demo Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://replicate.com/rinongal/stylegan-nada"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--&lt;!&ndash;                  <span>R </span>&ndash;&gt;-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-play"></i>-->
<!--                  </span>-->
<!--                  <span>Demo</span>-->
<!--                </a>-->
<!--              </span>-->


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--teaser-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images_perfusion/Teaser.png"
                   alt="Teaser."/>
      <h2 class="subtitle has-text-centered">
        (left) Perfusion enables image generation for personalized concepts with large changes in their appearance, pose, and context, using only 100KB of
        data and without compromising identity. (right) Perfusion can combine learned concepts at inference time, creating scenes which portray multiple concepts
        side-by-side, or even create interactions between them.
<!--        <br><br> Our work builds on the publicly available <a href="https://github.com/CompVis/latent-diffusion">Latent Diffusion Models</a>-->
      </h2>
    </div>
  </div>
</section>

<!--examples-->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="subtitle has-text-centered">Uncurated samples of image variations with text guided prompts.</h2>
      <div id="results-carousel-face" class="carousel results-carousel">

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/0.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/1.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/2.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/3.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/4.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/5.png"
                   alt="Puppet."/>
          </div>
        </div>

        <div class="item item-puppet">
          <div class="carousel-content">
            <img src="static/images_perfusion/slider_examples/6.png"
                   alt="Puppet."/>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<!--Abstract-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image models (T2I) offer a new level of flexibility by allowing users
            to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a
            challenging problem. The task of T2I personalization poses multiple hard
            challenges, such as maintaining high visual fidelity while allowing creative
            control, combining multiple personalized concepts in a single image, and
            keeping a small model size. We present Perfusion, a T2I personalization
            method that addresses these challenges using dynamic rank-1 updates to
            the underlying T2I model. Perfusion avoids overfitting by introducing a new
            mechanism that “locks” new concepts’ cross-attention Keys to their superordinate concept. Additionally, we develop a gated rank-1 approach that
            enables us to control the influence of a learned concept during inference time
            and to combine multiple concepts. This allows runtime efficient balancing
            of visual-fidelity and textual-alignment with a single 100KB trained model.
            Importantly, it can span different operating points across the Pareto front
            without additional training. We compare our approach to strong baselines
            and demonstrate its qualitative and quantitative strengths.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Architecture -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!--/ Architecture. -->
        <div class="section-title">
          <h2 class="title is-3 is-centered">How does it work?</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <div class="publication-img">
              <img id="architecture" src="static/images_perfusion/Architecture.png"/>
            </div>
          </div>
        </div>
        <p>
          Architecture outline (A): A prompt is transformed into a sequence of encodings. Each encoding is fed to a set of cross-attention modules (purple
blocks) of a diffusion U-Net denoiser. Zoomed-in purple module shows how the Key and Value pathways are conditioned on the text encoding. The Key drives
the attention map, which then modulates the Value pathway. Gated Rank-1 Edit (B): Top: The K pathway is locked so any encoding of 𝑒_Hugsy that reaches
𝑊𝑘 is mapped to the key of the supre-category 𝐾_teddy. Bottom: Any encoding of 𝑒_Hugsy that reaches 𝑊𝑣 , is mapped to 𝑉_Hugsy, which is learned
        </p>
    </div>
  </div>
</section>

<!-- Baseline compare -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Comparison To Current Methods</h2>
      </div>

    <div class="columns is-centered">

      <div class="column">
        <div class="content">

          <p>
            For each concept, we show exemplars from our training set, along with generated images,
their conditioning texts and comparisons to Custom-Diffusion (CD) and Dreambooth (DB) baselines. Perfusion can enable more animate results, with better
prompt-matching and less susceptibility to background traits from the original image.  <br>
          </p>
            <div class="publication-img">
              <img id="style_transfer" src="static/images_perfusion/baseline_comparison_single_concept.png"/>
            </div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Compositions -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Compositions</h2>
      </div>

    <div class="columns is-centered">

      <!-- Cross domain. -->
      <div class="column">
        <div class="content">

          <p>
            We show pairs of concepts interacting, and compare to CD. Except for the teddy* prompt, all prompts are from CD paper and use the images provided by the paper. <br>
          </p>
            <div class="publication-img">
              <img id="compositions" src="static/images_perfusion/compositions.png"/>
            </div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- One shot -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <br>
        <h2 class="title is-3">1-shot Personalization</h2>
      </div>

    <div class="columns is-centered">

      <!-- Cross domain. -->
      <div class="column">
        <div class="content">

          <p>
            When training with a single image, our
method can generate images with both high visual-fidelity and textual-
alignment.   <br>
          </p>
            <div class="publication-img">
              <img id="bias" src="static/images_perfusion/one_shot.png"/>
              <br><br>
            </div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Downstream models -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">

      <div class="section-title">
        <br>
        <h2 class="title is-3">Zero-shot Transfer To Fine-tuned Models</h2>
      </div>

    <div class="columns is-centered">

      <!-- Cross domain. -->
      <div class="column">
        <div class="content">

          <p>
            A Perfusion concept  trained using a vanilla diffusion-model can generalize to fine-tuned variants. <br>
          </p>
            <div class="publication-img">
              <img id="blended" src="static/images_perfusion/zero_shot_transfer.png"/>
               <br>
               <br>
            </div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!--&lt;!&ndash; Cite &ndash;&gt;-->
<!--<section class="section" id="BibTeX">-->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <p>If you find our work useful, please cite our paper:</p>-->
<!--    <pre><code>@misc{gal2022textual,-->
<!--      doi = {10.48550/ARXIV.2208.01618},-->
<!--      url = {https://arxiv.org/abs/2208.01618},-->
<!--      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},-->
<!--      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},-->
<!--      publisher = {arXiv},-->
<!--      year = {2022},-->
<!--      primaryClass={cs.CV}-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2208.01618">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/rinongal/textual_inversion" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
